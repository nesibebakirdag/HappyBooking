# Fabric notebook source

# METADATA ********************

# META {
# META   "kernel_info": {
# META     "name": "synapse_pyspark"
# META   },
# META   "dependencies": {
# META     "lakehouse": {
# META       "default_lakehouse": "300fcea7-917c-46f4-b882-033c12af9963",
# META       "default_lakehouse_name": "lh_happybooking",
# META       "default_lakehouse_workspace_id": "c3fa9526-c341-4b89-a11c-94170caf1f28",
# META       "known_lakehouses": [
# META         {
# META           "id": "300fcea7-917c-46f4-b882-033c12af9963"
# META         }
# META       ]
# META     },
# META     "environment": {
# META       "environmentId": "53e2ac77-9251-8735-4fc7-e25ce1360141",
# META       "workspaceId": "00000000-0000-0000-0000-000000000000"
# META     }
# META   }
# META }

# MARKDOWN ********************

# # AdÄ±m 6: Veri Kalite Testleri (Great Expectations)
# 
# **AmaÃ§:** `silver_bookings` tablosunun kalitesini doÄŸrulamak.
# 
# | Test | AÃ§Ä±klama |
# |------|----------|
# | Completeness | Kritik kolonlarda NULL yok |
# | Uniqueness | `booking_id` benzersiz |
# | Validity | Fiyatlar â‰¥ 0, tarihler geÃ§erli |
# | Type Check | Kolom tipleri doÄŸru |
# 
# **Not:** GX kurulumu iÃ§in kernel restart gerekebilir.

# CELL ********************

%pip install great_expectations==0.18.21 --quiet

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark",
# META   "frozen": true,
# META   "editable": false
# META }

# CELL ********************

import great_expectations as ge
from great_expectations.dataset import SparkDFDataset
import pyspark.sql.functions as F

print(f"âœ… Great Expectations version: {ge.__version__}")

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ## 1. Veriyi YÃ¼kle

# CELL ********************

df_silver = spark.read.table("silver_bookings")
gx_df = SparkDFDataset(df_silver)

print(f"âœ… silver_bookings yÃ¼klendi. SatÄ±r sayÄ±sÄ±: {df_silver.count()}")
print(f"Kolonlar: {df_silver.columns}")

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ## 2. Expectation'larÄ± TanÄ±mla

# CELL ********************

print("ğŸ§ª A. Completeness (Not Null) Testleri...")
gx_df.expect_column_values_to_not_be_null("booking_id")
gx_df.expect_column_values_to_not_be_null("customer_id")   # FIX: client_id â†’ customer_id
gx_df.expect_column_values_to_not_be_null("hotel_id")
gx_df.expect_column_values_to_not_be_null("booking_date")

print("ğŸ§ª B. Uniqueness Testleri...")
gx_df.expect_column_values_to_be_unique("booking_id")

print("ğŸ§ª C. Validity (Fiyat â‰¥ 0) Testleri...")
gx_df.expect_column_values_to_be_between("total_amount", min_value=0)
gx_df.expect_column_values_to_be_between("room_price", min_value=0)

print("ğŸ§ª D. Type Testleri...")
gx_df.expect_column_values_to_be_of_type("booking_id", "StringType")
gx_df.expect_column_values_to_be_of_type("total_amount", "DoubleType")
gx_df.expect_column_values_to_be_of_type("customer_id", "StringType")

print("ğŸ§ª E. Domain Testleri...")
gx_df.expect_column_values_to_be_in_set(
    "currency",
    ["EUR", "USD", "GBP", "JPY", "TRY", "AED", "CNY"]
)

print("âœ… TÃ¼m expectation'lar tanÄ±mlandÄ±.")

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ## 3. Validasyon Ã‡alÄ±ÅŸtÄ±r & Rapor

# CELL ********************

print("ğŸ“Š Full Validation Suite Ã§alÄ±ÅŸÄ±yor...")
results = gx_df.validate()

# Ã–zet
total_tests  = len(results["results"])
passed_tests = sum(1 for r in results["results"] if r["success"])
failed_tests = total_tests - passed_tests

print(f"\n{'='*50}")
print(f"ğŸ“‹ VALIDATION REPORT â€” silver_bookings")
print(f"{'='*50}")
print(f"  Toplam Test  : {total_tests}")
print(f"  âœ… GeÃ§ti     : {passed_tests}")
print(f"  âŒ KaldÄ±     : {failed_tests}")
print(f"{'='*50}")

if results["success"]:
    print("ğŸ‰ SUCCESS! TÃ¼m kalite kontrolleri geÃ§ti.")
else:
    print("âš ï¸ BazÄ± testler baÅŸarÄ±sÄ±z:")
    for res in results["results"]:
        status = "âœ…" if res["success"] else "âŒ"
        exp_type = res["expectation_config"]["expectation_type"]
        col_name = res["expectation_config"]["kwargs"].get("column", "")
        print(f"  {status} {exp_type}({col_name})")
        if not res["success"] and "result" in res:
            print(f"       â†’ {res['result']}")

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ## 4. Native PySpark Kalite Ã–zeti
# 
# GX kurulumu baÅŸarÄ±sÄ±z olursa bu cell'i Ã§alÄ±ÅŸtÄ±r.

# CELL ********************

from pyspark.sql.functions import col, count, when, isnan, isnull

df = spark.read.table("silver_bookings")
total = df.count()

checks = [
    ("booking_id NULL",    df.filter(col("booking_id").isNull()).count()),
    ("customer_id NULL",   df.filter(col("customer_id").isNull()).count()),
    ("hotel_id NULL",      df.filter(col("hotel_id").isNull()).count()),
    ("booking_date NULL",  df.filter(col("booking_date").isNull()).count()),
    ("total_amount < 0",   df.filter(col("total_amount") < 0).count()),
    ("city_clean NULL",    df.filter(col("city_clean").isNull()).count()),
    ("duplicate booking",  total - df.select("booking_id").distinct().count()),
]

print(f"\n{'='*55}")
print(f"{'TEST':<30} {'SORUNLU':>10} {'DURUM':>10}")
print(f"{'='*55}")
for name, count_val in checks:
    status = "âœ… OK" if count_val == 0 else f"âŒ {count_val:,}"
    print(f"{name:<30} {count_val:>10,} {status:>10}")
print(f"{'='*55}")
print(f"Toplam satÄ±r: {total:,}")

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }
